{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN using coronary heart disease data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This work is aiming at the design and testing of a deep neural network architecture on Coronary heart disease using Tensorflow and Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy.random import seed\n",
    "np.random.seed(2095)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['SEQN', 'Gender', 'Age', 'Annual-Family-Income',\n",
      "       'Ratio-Family-Income-Poverty', 'X60-sec-pulse', 'Systolic', 'Diastolic',\n",
      "       'Weight', 'Height', 'Body-Mass-Index', 'White-Blood-Cells',\n",
      "       'Lymphocyte', 'Monocyte', 'Eosinophils', 'Basophils', 'Red-Blood-Cells',\n",
      "       'Hemoglobin', 'Mean-Cell-Vol', 'Mean-Cell-Hgb-Conc.',\n",
      "       'Mean-cell-Hemoglobin', 'Platelet-count', 'Mean-Platelet-Vol',\n",
      "       'Segmented-Neutrophils', 'Hematocrit', 'Red-Cell-Distribution-Width',\n",
      "       'Albumin', 'ALP', 'AST', 'ALT', 'Cholesterol', 'Creatinine', 'Glucose',\n",
      "       'GGT', 'Iron', 'LDH', 'Phosphorus', 'Bilirubin', 'Protein', 'Uric.Acid',\n",
      "       'Triglycerides', 'Total-Cholesterol', 'HDL', 'Glycohemoglobin',\n",
      "       'Vigorous-work', 'Moderate-work', 'Health-Insurance', 'Diabetes',\n",
      "       'Blood-Rel-Diabetes', 'Blood-Rel-Stroke', 'CoronaryHeartDisease'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# read input file\n",
    "file = 'C:\\\\Users\\\\rohan\\\\OneDrive\\\\Documents\\\\CardiacPrediction_modified.xlsx'\n",
    "#ipData = pd.read_excel(file, sheet_name='Stroke')\n",
    "ipData = pd.read_excel(file, sheet_name='CoroHeartDis')\n",
    "print(ipData.columns) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data drop\n",
    "opLabel = np.array(ipData['CoronaryHeartDisease'])\n",
    "ipData.drop(['SEQN','CoronaryHeartDisease','Annual-Family-Income','Height','Ratio-Family-Income-Poverty','X60-sec-pulse',\n",
    "          'Health-Insurance','Lymphocyte','Monocyte','Eosinophils','Total-Cholesterol','Mean-Cell-Vol','Mean-Cell-Hgb-Conc.','Hematocrit','Segmented-Neutrophils'], axis = 1, inplace=True)\n",
    "\n",
    "\n",
    "#opLabel = np.array(ipData['Stroke'])\n",
    "#ipData.drop(['SEQN','Stroke','Annual-Family-Income','Height','Ratio-Family-Income-Poverty','X60-sec-pulse',\n",
    "#          'Health-Insurance','Lymphocyte','Monocyte','Eosinophils','Total-Cholesterol','Mean-Cell-Vol','Mean-Cell-Hgb-Conc.','Hematocrit','Segmented-Neutrophils'], axis = 1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy variable for categorical variables\n",
    "ipData = pd.get_dummies(ipData,columns=[\"Gender\",\"Diabetes\",\"Blood-Rel-Diabetes\",\"Blood-Rel-Stroke\",\"Vigorous-work\",\"Moderate-work\"]) \n",
    "\n",
    "varb = np.array(ipData.columns)\n",
    "ipData = np.array(ipData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37079, 45)\n",
      "['Age' 'Systolic' 'Diastolic' 'Weight' 'Body-Mass-Index'\n",
      " 'White-Blood-Cells' 'Basophils' 'Red-Blood-Cells' 'Hemoglobin'\n",
      " 'Mean-cell-Hemoglobin' 'Platelet-count' 'Mean-Platelet-Vol'\n",
      " 'Red-Cell-Distribution-Width' 'Albumin' 'ALP' 'AST' 'ALT' 'Cholesterol'\n",
      " 'Creatinine' 'Glucose' 'GGT' 'Iron' 'LDH' 'Phosphorus' 'Bilirubin'\n",
      " 'Protein' 'Uric.Acid' 'Triglycerides' 'HDL' 'Glycohemoglobin' 'Gender_1'\n",
      " 'Gender_2' 'Diabetes_1' 'Diabetes_2' 'Diabetes_3' 'Blood-Rel-Diabetes_1'\n",
      " 'Blood-Rel-Diabetes_2' 'Blood-Rel-Stroke_1' 'Blood-Rel-Stroke_2'\n",
      " 'Vigorous-work_1' 'Vigorous-work_2' 'Vigorous-work_3' 'Moderate-work_1'\n",
      " 'Moderate-work_2' 'Moderate-work_3']\n",
      "35571\n",
      "1508\n"
     ]
    }
   ],
   "source": [
    "print(ipData.shape)\n",
    "print(varb)\n",
    "print(len(opLabel[opLabel==0]))\n",
    "print(len(opLabel[opLabel==1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Nomination using Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge  # Import Ridge\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "featureVote = np.zeros(ipData.shape[1])\n",
    "print(featureVote.shape)\n",
    "\n",
    "# Calculate sizes for train, test, and validation sets\n",
    "total_size = ipData.shape[0]\n",
    "train_size = int(0.70 * total_size)\n",
    "test_size = val_size = int(0.15 * total_size)\n",
    "iteR=100\n",
    "for num in range(iteR):\n",
    "    # Shuffle the indices for each label\n",
    "    label0_indx = np.where(opLabel == 0)[0]\n",
    "    label1_indx = np.where(opLabel == 1)[0]\n",
    "    np.random.shuffle(label0_indx)\n",
    "    np.random.shuffle(label1_indx)\n",
    "\n",
    "    # Split indices into train, test, and validation sets for each label\n",
    "    label0_indx_train = label0_indx[:train_size // 2]\n",
    "    label1_indx_train = label1_indx[:train_size // 2]\n",
    "    label0_indx_test = label0_indx[train_size // 2:train_size // 2 + test_size // 2]\n",
    "    label1_indx_test = label1_indx[train_size // 2:train_size // 2 + test_size // 2]\n",
    "    label0_indx_val = label0_indx[train_size // 2 + test_size // 2:train_size // 2 + test_size // 2 + val_size // 2]\n",
    "    label1_indx_val = label1_indx[train_size // 2 + test_size // 2:train_size // 2 + test_size // 2 + val_size // 2]\n",
    "\n",
    "    # Combine indices for each set\n",
    "    trainIndx = np.append(label0_indx_train, label1_indx_train)\n",
    "    testIndx = np.append(label0_indx_test, label1_indx_test)\n",
    "    valIndx = np.append(label0_indx_val, label1_indx_val)\n",
    "\n",
    "    # Select data for each set\n",
    "    trainData = ipData[trainIndx]\n",
    "    trainLabel = opLabel[trainIndx]\n",
    "    testData = ipData[testIndx]\n",
    "    testLabel = opLabel[testIndx]\n",
    "    valData = ipData[valIndx]\n",
    "    valLabel = opLabel[valIndx]\n",
    "\n",
    "    # Data standardization\n",
    "    scaler = preprocessing.StandardScaler().fit(trainData)\n",
    "    trainData_scaled = scaler.transform(trainData)\n",
    "    testData_scaled = scaler.transform(testData)\n",
    "    valData_scaled = scaler.transform(valData)\n",
    "\n",
    "    # Use Ridge instead of Lasso\n",
    "    regr = Ridge(random_state=0, alpha=0.006, tol=0.000001, max_iter=100000)\n",
    "    regr.fit(trainData_scaled, trainLabel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "<built-in function iter>\n"
     ]
    }
   ],
   "source": [
    "print(featureVote)\n",
    "print(iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Age' 'Systolic' 'Diastolic' 'Weight' 'Body-Mass-Index'\n",
      " 'White-Blood-Cells' 'Basophils' 'Red-Blood-Cells' 'Hemoglobin'\n",
      " 'Mean-cell-Hemoglobin' 'Platelet-count' 'Mean-Platelet-Vol'\n",
      " 'Red-Cell-Distribution-Width' 'Albumin' 'ALP' 'AST' 'ALT' 'Cholesterol'\n",
      " 'Creatinine' 'Glucose' 'GGT' 'Iron' 'LDH' 'Phosphorus' 'Bilirubin'\n",
      " 'Protein' 'Uric.Acid' 'Triglycerides' 'HDL' 'Glycohemoglobin' 'Gender_1'\n",
      " 'Gender_2' 'Diabetes_1' 'Diabetes_2' 'Diabetes_3' 'Blood-Rel-Diabetes_1'\n",
      " 'Blood-Rel-Diabetes_2' 'Blood-Rel-Stroke_1' 'Blood-Rel-Stroke_2'\n",
      " 'Vigorous-work_1' 'Vigorous-work_2' 'Vigorous-work_3' 'Moderate-work_1'\n",
      " 'Moderate-work_2' 'Moderate-work_3']\n"
     ]
    }
   ],
   "source": [
    "# feature nomination via Lasso (from feature 1 to 30)\n",
    "# We keep the dummy variables\n",
    "\n",
    "#thresH = iteR//5. Pick features occuring more than 5 times\n",
    "thresH = 0\n",
    "featureIndx = np.where(featureVote[0:30]>=thresH)[0]\n",
    "featureIndx = np.append(featureIndx, np.arange(30,ipData.shape[1]))\n",
    "print(varb[featureIndx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureIndx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "tInx = np.arange(ipData.shape[1])\n",
    "rrInx = tInx[~np.isin(tInx,featureIndx)]\n",
    "print(varb[rrInx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset reduction (Re-run every time you change the number of training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data = ipData[:,featureIndx]\n",
    "\n",
    "label0_indx = np.where(opLabel==0)[0]   #no cardiac arrest\n",
    "label1_indx = np.where(opLabel==1)[0]   #cardiac arrest \n",
    "\n",
    "# Calculate sizes for train, test, and validation sets\n",
    "total_size0 = len(label0_indx)\n",
    "total_size1 = len(label1_indx)\n",
    "train_size0 = int(0.70 * total_size0)\n",
    "train_size1 = int(0.70 * total_size1)\n",
    "val_size0 = test_size0 = int(0.15 * total_size0)\n",
    "val_size1 = test_size1 = int(0.15 * total_size1)\n",
    "\n",
    "np.random.shuffle(label0_indx)\n",
    "np.random.shuffle(label1_indx)\n",
    "\n",
    "# Splitting indices into train, validation, and test sets\n",
    "label0_indx_train = label0_indx[:train_size0]\n",
    "label1_indx_train = label1_indx[:train_size1]\n",
    "label0_indx_val = label0_indx[train_size0:train_size0 + val_size0]\n",
    "label1_indx_val = label1_indx[train_size1:train_size1 + val_size1]\n",
    "label0_indx_test = label0_indx[train_size0 + val_size0:]\n",
    "label1_indx_test = label1_indx[train_size1 + val_size1:]\n",
    "\n",
    "# Combine indices for each set\n",
    "trainIndx = np.append(label0_indx_train, label1_indx_train)\n",
    "valIndx = np.append(label0_indx_val, label1_indx_val)\n",
    "testIndx = np.append(label0_indx_test, label1_indx_test)\n",
    "\n",
    "# Select data for each set\n",
    "x_train = reduced_data[trainIndx]\n",
    "y_train = opLabel[trainIndx]\n",
    "x_val = reduced_data[valIndx]\n",
    "y_val = opLabel[valIndx]\n",
    "x_test = reduced_data[testIndx]\n",
    "y_test = opLabel[testIndx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5337\n"
     ]
    }
   ],
   "source": [
    "print(len(label0_indx_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import copy\n",
    "\n",
    "# Scale the data\n",
    "scaler = preprocessing.StandardScaler().fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_val = scaler.transform(x_val)  # Scale the validation data\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "# Create deep copies of the original scaled data\n",
    "x_train_org = copy.deepcopy(x_train)\n",
    "x_val_org = copy.deepcopy(x_val)  # Copy for validation data\n",
    "x_test_org = copy.deepcopy(x_test)\n",
    "y_train_org = copy.deepcopy(y_train)\n",
    "y_val_org = copy.deepcopy(y_val)  # Copy for validation labels\n",
    "y_test_org = copy.deepcopy(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE visualization\n",
    "import imblearn\n",
    "from collections import Counter\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohan\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:780: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rohan\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 25954 samples in 0.003s...\n"
     ]
    }
   ],
   "source": [
    "X_embedded = TSNE(n_components=3, n_iter = 300, verbose=1).fit_transform(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "cin = sns.color_palette(\"Set1\")[1]\n",
    "edge_color = []\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_facecolor('w')\n",
    "ax.scatter(X_embedded[:,0],X_embedded[:,1],X_embedded[:,2],c=[sns.color_palette(\"Set1\")[x] for x in y_train],\\\n",
    "           edgecolors='k',label='no-CHD')\n",
    "ax.scatter(0,0,0,c=cin,edgecolors='k',label='CHD')\n",
    "ax.azim = 20\n",
    "ax.elev = 20\n",
    "ax.set_xlabel(\"t-SNE Dim 1\", size=\"x-large\")\n",
    "ax.set_ylabel(\"t-SNE Dim 2\", size=\"x-large\")\n",
    "ax.set_zlabel(\"t-SNE Dim 3\", size=\"x-large\")\n",
    "plt.title(\"Random subsampling 3:1\",size=\"xx-large\")\n",
    "ax.legend(loc='upper left')\n",
    "fig.set_size_inches(7.5,7.5)\n",
    "fig.savefig('tSNE_RUS.png', dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0],x_train.shape[1],1)\n",
    "x_test = x_test.reshape(x_test.shape[0],x_test.shape[1],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% one-hot-encoding\n",
    "y_train = keras.utils.to_categorical(y_train,2)\n",
    "y_test  = keras.utils.to_categorical(y_test,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train.shape\n",
    "#y_train\n",
    "y_train_org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the part above is MANDATORY for training any Network below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing convolutional layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP+Conv :: Input => Hidden(128)  => Conv(4) => Output (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs = keras.layers.Input(shape=(x_train.shape[1],1))\n",
    "RS0    = keras.layers.Reshape((x_train.shape[1], ))(inputs)\n",
    "FC0    = keras.layers.Dense(128, bias_initializer=keras.initializers.VarianceScaling())(RS0)\n",
    "BN0    = keras.layers.BatchNormalization(axis=-1)(FC0)\n",
    "AC0    = keras.layers.Activation('relu')(BN0)\n",
    "DP0    = keras.layers.Dropout(0.2)(AC0)\n",
    "\n",
    "RS1    = keras.layers.Reshape((128,1))(DP0)\n",
    "FC1    = keras.layers.Conv1D(4,3,strides=1)(RS1)\n",
    "BN1    = keras.layers.BatchNormalization(axis=-1)(FC1)\n",
    "AC1    = keras.layers.Activation('relu')(BN1)\n",
    "Pool1 = keras.layers.MaxPool1D(pool_size=2)(AC1)\n",
    "FL1   = keras.layers.Flatten()(Pool1)\n",
    "\n",
    "FC3 =   keras.layers.Dense(2, bias_initializer=keras.initializers.VarianceScaling())(FL1)\n",
    "outputs = keras.layers.Activation('softmax')(FC3)\n",
    "\n",
    "myCNN1D1 = keras.Model(inputs=inputs,outputs=outputs)\n",
    "myCNN1D1.compile(optimizer=keras.optimizers.Adam(),\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "myCNN1D1.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_weight = {0: 1, 1: 4}\n",
    "\n",
    "myCNN1D1.fit(x_train,y_train,epochs=1,verbose=1, class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss,test_acc = myCNN1D1.evaluate(x_test,y_test)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "predlabel = myCNN1D1.predict(x_test)\n",
    "f = np.argmax(predlabel,axis=1)\n",
    "confMat = metrics.confusion_matrix(np.argmax(y_test,axis=1),f)\n",
    "print(confMat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  MLP+CONV: Input => Hidden(128)  => Conv(4) => Conv(8) => Output (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.layers.Input(shape=(x_train.shape[1],1))\n",
    "RS0    = keras.layers.Reshape((x_train.shape[1], ))(inputs)\n",
    "FC0    = keras.layers.Dense(128, bias_initializer=keras.initializers.VarianceScaling())(RS0)\n",
    "BN0    = keras.layers.BatchNormalization(axis=-1)(FC0)\n",
    "AC0    = keras.layers.Activation('relu')(BN0)\n",
    "DP0    = keras.layers.Dropout(0.2)(AC0)\n",
    "\n",
    "RS1    = keras.layers.Reshape((128,1))(DP0)\n",
    "FC1    = keras.layers.Conv1D(4,3,strides=1)(RS1)\n",
    "BN1    = keras.layers.BatchNormalization(axis=-1)(FC1)\n",
    "AC1    = keras.layers.Activation('relu')(BN1)\n",
    "Pool1 = keras.layers.AveragePooling1D(pool_size=2)(AC1)\n",
    "\n",
    "FC2    = keras.layers.Conv1D(8,5,strides=1)(Pool1)\n",
    "BN2    = keras.layers.BatchNormalization(axis=-1)(FC2)\n",
    "AC2    = keras.layers.Activation('relu')(BN2)\n",
    "Pool2 = keras.layers.AveragePooling1D(pool_size=2)(AC2)\n",
    "\n",
    "FL1   = keras.layers.Flatten()(Pool2)\n",
    "\n",
    "FC3 =   keras.layers.Dense(2, bias_initializer=keras.initializers.VarianceScaling())(FL1)\n",
    "outputs = keras.layers.Activation('softmax')(FC3)\n",
    "\n",
    "myCNN1D2 = keras.Model(inputs=inputs,outputs=outputs)\n",
    "myCNN1D2.compile(optimizer=keras.optimizers.Adam(),\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "myCNN1D2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class_weight = {0: 1, 1: 4.5}\n",
    "\n",
    "myCNN1D2.fit(x_train,y_train,epochs=1,verbose=1, class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_loss,test_acc = myCNN1D2.evaluate(x_test,y_test)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "predlabel = myCNN1D2.predict(x_test)\n",
    "f = np.argmax(predlabel,axis=1)\n",
    "confMat = metrics.confusion_matrix(np.argmax(y_test,axis=1),f)\n",
    "print(confMat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP+Conv :: Input => Hidden(64) => Conv(2) => Hidden(512)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "inputs = keras.layers.Input(shape=(x_train.shape[1],1))\n",
    "RS0    = keras.layers.Reshape((x_train.shape[1], ))(inputs)\n",
    "FC0    = keras.layers.Dense(32, bias_initializer=keras.initializers.VarianceScaling())(RS0)\n",
    "BN0    = keras.layers.BatchNormalization(axis=-1)(FC0)\n",
    "AC0    = keras.layers.Activation('relu')(BN0)\n",
    "DP0    = keras.layers.Dropout(0.2)(AC0)\n",
    "\n",
    "RS1    = keras.layers.Reshape((32,1))(DP0)\n",
    "FC1    = keras.layers.Conv1D(2,3,strides=1)(RS1)\n",
    "BN1    = keras.layers.BatchNormalization(axis=-1)(FC1)\n",
    "AC1    = keras.layers.Activation('relu')(BN1)\n",
    "Pool1 = keras.layers.MaxPool1D(pool_size=2)(AC1)\n",
    "FL1   = keras.layers.Flatten()(Pool1)\n",
    "\n",
    "FC2    = keras.layers.Dense(512, bias_initializer=keras.initializers.VarianceScaling())(FL1)\n",
    "BN2    = keras.layers.BatchNormalization(axis=-1)(FC2)\n",
    "AC2    = keras.layers.Activation('relu')(BN2)\n",
    "#DP2    = keras.layers.Dropout(0.2)(AC2)\n",
    "\n",
    "\n",
    "FC3 =   keras.layers.Dense(2, bias_initializer=keras.initializers.VarianceScaling())(AC2)\n",
    "outputs = keras.layers.Activation('softmax')(FC3)\n",
    "\n",
    "myCNN1D3 = keras.Model(inputs=inputs,outputs=outputs)\n",
    "myCNN1D3.compile(optimizer=keras.optimizers.Adam(),\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "myCNN1D3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_weight = {0: 1, 1: 50}\n",
    "\n",
    "myCNN1D3.fit(x_train,y_train,epochs=5,verbose=1, class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_loss,test_acc = myCNN1D3.evaluate(x_test,y_test)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "predlabel = myCNN1D3.predict(x_test)\n",
    "f = np.argmax(predlabel,axis=1)\n",
    "confMat = metrics.confusion_matrix(np.argmax(y_test,axis=1),f)\n",
    "print(confMat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP+ CONV: Input => Dense(64) => Conv(2) => Conv(4) = Dense(512) => Dense (2) [Best model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.layers.Input(shape=(x_train.shape[1],1))\n",
    "RS0    = keras.layers.Reshape((x_train.shape[1], ))(inputs)\n",
    "FC0    = keras.layers.Dense(64, bias_initializer=keras.initializers.VarianceScaling())(RS0)\n",
    "BN0    = keras.layers.BatchNormalization(axis=-1)(FC0)\n",
    "AC0    = keras.layers.Activation('relu')(BN0)\n",
    "DP0    = keras.layers.Dropout(0.2)(AC0)\n",
    "\n",
    "RS1    = keras.layers.Reshape((64,1))(DP0)\n",
    "FC1    = keras.layers.Conv1D(2,3,strides=1)(RS1)\n",
    "BN1    = keras.layers.BatchNormalization(axis=-1)(FC1)\n",
    "AC1    = keras.layers.Activation('relu')(BN1)\n",
    "Pool1  = keras.layers.AveragePooling1D(pool_size=2)(AC1)\n",
    "\n",
    "FC2    = keras.layers.Conv1D(4,5,strides=1)(Pool1)\n",
    "BN2    = keras.layers.BatchNormalization(axis=-1)(FC2)\n",
    "AC2    = keras.layers.Activation('relu')(BN2)\n",
    "Pool2  = keras.layers.AveragePooling1D(pool_size=2)(AC2)\n",
    "\n",
    "FL1    = keras.layers.Flatten()(Pool2)\n",
    "\n",
    "FC3    = keras.layers.Dense(512, bias_initializer=keras.initializers.VarianceScaling())(FL1)\n",
    "BN3    = keras.layers.BatchNormalization(axis=-1)(FC3)\n",
    "AC3    = keras.layers.Activation('relu')(BN3)\n",
    "DP3    = keras.layers.Dropout(0.2)(AC3)\n",
    "\n",
    "\n",
    "FC4    = keras.layers.Dense(2)(DP3)\n",
    "outputs = keras.layers.Activation('softmax')(FC4)\n",
    "\n",
    "myCNN1D4 = keras.Model(inputs=inputs,outputs=outputs)\n",
    "myCNN1D4.compile(optimizer=keras.optimizers.Adam(),\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "#myCNN1D4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_weight = {0: 1, 1: 2.9}\n",
    "\n",
    "myCNN1D4.fit(x_train,y_train,epochs=1,verbose=1, class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_loss,test_acc = myCNN1D4.evaluate(x_test,y_test)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "predlabel = myCNN1D4.predict(x_test)\n",
    "f = np.argmax(predlabel,axis=1)\n",
    "confMat = metrics.confusion_matrix(np.argmax(y_test,axis=1),f)\n",
    "print(confMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train @ np.transpose(np.array([0, 1]))\n",
    "1/41"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input => Dense(64) => Conv(2) => Conv(4) = Conv(8) => Dense (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.layers.Input(shape=(x_train.shape[1],1))\n",
    "RS0    = keras.layers.Reshape((x_train.shape[1], ))(inputs)\n",
    "FC0    = keras.layers.Dense(64, bias_initializer=keras.initializers.VarianceScaling())(RS0)\n",
    "BN0    = keras.layers.BatchNormalization(axis=-1)(FC0)\n",
    "AC0    = keras.layers.Activation('relu')(BN0)\n",
    "DP0    = keras.layers.Dropout(0.2)(AC0)\n",
    "\n",
    "RS1    = keras.layers.Reshape((64,1))(DP0)\n",
    "FC1    = keras.layers.Conv1D(2,3,strides=1)(RS1)\n",
    "BN1    = keras.layers.BatchNormalization(axis=-1)(FC1)\n",
    "AC1    = keras.layers.Activation('relu')(BN1)\n",
    "Pool1  = keras.layers.AveragePooling1D(pool_size=2)(AC1)\n",
    "\n",
    "FC2    = keras.layers.Conv1D(4,5,strides=1)(Pool1)\n",
    "BN2    = keras.layers.BatchNormalization(axis=-1)(FC2)\n",
    "AC2    = keras.layers.Activation('relu')(BN2)\n",
    "Pool2  = keras.layers.AveragePooling1D(pool_size=2)(AC2)\n",
    "\n",
    "\n",
    "FC3    = keras.layers.Conv1D(6,7,strides=1)(Pool2)\n",
    "BN3    = keras.layers.BatchNormalization(axis=-1)(FC3)\n",
    "AC3    = keras.layers.Activation('relu')(BN3)\n",
    "Pool3  = keras.layers.AveragePooling1D(pool_size=2)(AC3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "FL1    = keras.layers.Flatten()(Pool3)\n",
    "\n",
    "#FC3    = keras.layers.Dense(512, bias_initializer=keras.initializers.VarianceScaling())(FL1)\n",
    "#BN3    = keras.layers.BatchNormalization(axis=-1)(FC3)\n",
    "#AC3    = keras.layers.Activation('relu')(BN3)\n",
    "#DP3    = keras.layers.Dropout(0.2)(AC3)\n",
    "\n",
    "\n",
    "FC4    = keras.layers.Dense(2)(FL1)\n",
    "outputs = keras.layers.Activation('softmax')(FC4)\n",
    "\n",
    "myCNN5D4 = keras.Model(inputs=inputs,outputs=outputs)\n",
    "myCNN5D4.compile(optimizer=keras.optimizers.Adam(),\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "myCNN5D4.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = {0: 1, 1: 3.0}\n",
    "\n",
    "myCNN5D4.fit(x_train,y_train,epochs=1,verbose=1, class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss,test_acc = myCNN5D4.evaluate(x_test,y_test)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "predlabel = myCNN5D4.predict(x_test)\n",
    "f = np.argmax(predlabel,axis=1)\n",
    "confMat = metrics.confusion_matrix(np.argmax(y_test,axis=1),f)\n",
    "print(confMat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input => Dense(64) => Conv(2) => Dense (128) = Conv(4) => Dense (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.layers.Input(shape=(x_train.shape[1],1))\n",
    "RS0    = keras.layers.Reshape((x_train.shape[1], ))(inputs)\n",
    "FC0    = keras.layers.Dense(64, bias_initializer=keras.initializers.VarianceScaling())(RS0)\n",
    "BN0    = keras.layers.BatchNormalization(axis=-1)(FC0)\n",
    "AC0    = keras.layers.Activation('relu')(BN0)\n",
    "DP0    = keras.layers.Dropout(0.2)(AC0)\n",
    "\n",
    "RS1    = keras.layers.Reshape((64,1))(DP0)\n",
    "FC1    = keras.layers.Conv1D(2,3,strides=1)(RS1)\n",
    "BN1    = keras.layers.BatchNormalization(axis=-1)(FC1)\n",
    "AC1    = keras.layers.Activation('relu')(BN1)\n",
    "Pool1  = keras.layers.AveragePooling1D(pool_size=2)(AC1)\n",
    "\n",
    "FL0 = keras.layers.Flatten()(Pool1)\n",
    "\n",
    "AA1 = keras.layers.Dense(128, bias_initializer=keras.initializers.VarianceScaling())(FL0)\n",
    "BB1 = keras.layers.BatchNormalization(axis=-1)(AA1)\n",
    "CC1 = keras.layers.Activation('relu')(BB1)\n",
    "Pool2 = keras.layers.Dropout(0.2)(CC1)\n",
    "\n",
    "\n",
    "RS2    = keras.layers.Reshape((128,1))(Pool2)\n",
    "FC2    = keras.layers.Conv1D(4,5,strides=1)(RS2)\n",
    "BN2    = keras.layers.BatchNormalization(axis=-1)(FC2)\n",
    "AC2    = keras.layers.Activation('relu')(BN2)\n",
    "Pool3  = keras.layers.AveragePooling1D(pool_size=2)(AC2)\n",
    "\n",
    "\n",
    "\n",
    "FL1    = keras.layers.Flatten()(Pool3)\n",
    "\n",
    "#FC3    = keras.layers.Dense(512, bias_initializer=keras.initializers.VarianceScaling())(FL1)\n",
    "#BN3    = keras.layers.BatchNormalization(axis=-1)(FC3)\n",
    "#AC3    = keras.layers.Activation('relu')(BN3)\n",
    "#DP3    = keras.layers.Dropout(0.2)(AC3)\n",
    "\n",
    "\n",
    "FC4    = keras.layers.Dense(2)(FL1)\n",
    "outputs = keras.layers.Activation('softmax')(FC4)\n",
    "\n",
    "myCNN5D4 = keras.Model(inputs=inputs,outputs=outputs)\n",
    "myCNN5D4.compile(optimizer=keras.optimizers.Adam(),\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "myCNN5D4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_weight = {0: 1, 1: 2.5}\n",
    "\n",
    "myCNN5D4.fit(x_train,y_train,epochs=1,verbose=1, class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss,test_acc = myCNN5D4.evaluate(x_test,y_test)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "predlabel = myCNN5D4.predict(x_test)\n",
    "f = np.argmax(predlabel,axis=1)\n",
    "confMat = metrics.confusion_matrix(np.argmax(y_test,axis=1),f)\n",
    "print(confMat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running SVM:Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid={'C':[0.01,0.05,0.1,1],'gamma':[0.1,1/41,0.01,0.001],'kernel':['rbf','poly','sigmoid']}\n",
    "grid=GridSearchCV(SVC(),param_grid,refit=True,verbose=2)\n",
    "grid.fit(x_val_org,y_val_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "\n",
    "svcCAD = SVC(C=0.01, gamma=0.1, kernel='poly', class_weight='balanced')\n",
    "svcCAD.fit(x_train_org, y_train_org)\n",
    "predlabel = svcCAD.predict(x_val_org)\n",
    "\n",
    "# Confusion matrix77\n",
    "confMat = metrics.confusion_matrix(y_val_org, predlabel)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confMat)\n",
    "\n",
    "# Calculating accuracy\n",
    "accuracy = metrics.accuracy_score(y_val_org, predlabel)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running SVM:Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "\n",
    "svcCAD = SVC(C=0.01, gamma=0.1, kernel='poly', class_weight='balanced')\n",
    "svcCAD.fit(x_train_org, y_train_org)\n",
    "predlabel = svcCAD.predict(x_test_org)\n",
    "\n",
    "# Confusion matrix\n",
    "confMat = metrics.confusion_matrix(y_test_org, predlabel)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confMat)\n",
    "\n",
    "# Calculating accuracy\n",
    "accuracy = metrics.accuracy_score(y_test_org, predlabel)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running KNN:Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "k=1\n",
    "for i in range(5):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)  \n",
    "    knn.fit(trainData_scaled, trainLabel)  # Use the training data\n",
    "    predlabel = knn.predict(valData_scaled)  # Predict on the validation set\n",
    "\n",
    "        # Confusion matrix\n",
    "    confMat = metrics.confusion_matrix(valLabel, predlabel)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confMat)\n",
    "\n",
    "        # Calculating accuracy\n",
    "    accuracy = metrics.accuracy_score(valLabel, predlabel)\n",
    "    print(\"Accuracy: {:.2f}%\".format(accuracy * 100), k)\n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running KNN:Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Running KNN on the subsampled data\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=4)  \n",
    "knn.fit(x_train_org, y_train_org)\n",
    "predlabel = knn.predict(x_test_org)\n",
    "\n",
    "# Confusion matrix\n",
    "confMat = metrics.confusion_matrix(y_test_org, predlabel)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confMat)\n",
    "\n",
    "# Calculating accuracy\n",
    "accuracy = metrics.accuracy_score(y_test_org, predlabel)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Vote:Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid={'criterion':[\"gini\",\"entropy\"],'max_depth':[1000,10000,100000,1000000]}\n",
    "grid=GridSearchCV(DecisionTreeClassifier(),param_grid,refit=True,verbose=2)\n",
    "grid.fit(x_val_org,y_val_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Assuming valData_scaled and valLabel are defined and prepared\n",
    "svm = SVC(C=0.01, gamma=0.1, kernel='poly', probability=True)\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "voting_clf = VotingClassifier(estimators=[('knn', knn), ('svm', svm), ('dt', dt)], voting='hard')\n",
    "\n",
    "voting_clf.fit(trainData_scaled, trainLabel)  # Use the training data\n",
    "predlabel = voting_clf.predict(valData_scaled)  # Predict on the validation set\n",
    "\n",
    "# Confusion matrix\n",
    "confMat = metrics.confusion_matrix(valLabel, predlabel)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confMat)\n",
    "\n",
    "# Calculating accuracy\n",
    "accuracy = metrics.accuracy_score(valLabel, predlabel)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Vote: Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=4)\n",
    "svm = SVC(C=0.01, gamma=0.1, kernel='poly', probability=True)  # probability must be True for soft voting\n",
    "dt = DecisionTreeClassifier(criterion='entropy', max_depth=100000)\n",
    "\n",
    "# Create a voting classifier\n",
    "voting_clf = VotingClassifier(estimators=[('knn', knn), ('svm', svm), ('dt', dt)], voting='hard') \n",
    "voting_clf.fit(x_train_org, y_train_org)\n",
    "predlabel = voting_clf.predict(x_test_org)\n",
    "\n",
    "# Confusion matrix\n",
    "confMat = metrics.confusion_matrix(y_test_org, predlabel)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confMat)\n",
    "\n",
    "# Calculating accuracy\n",
    "accuracy = metrics.accuracy_score(y_test_org, predlabel)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
